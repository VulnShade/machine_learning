# ğŸ“ˆ Linear Regression

[â† Back to Main Page](../../README.md) | [â† Back to Supervised Learning](../supervised_learning.md)

> Predicting continuous values using linear relationships

## ğŸ“‹ Contents
- [Single Variable Linear Regression](#single-variable-linear-regression)
- [Multiple Linear Regression](#multiple-linear-regression)
- [Feature Scaling](#feature-scaling)
- [Checking Gradient Descent Convergence](#checking-gradient-decent-for-convergence)

## ğŸ“ Key Concepts
- Model trained on labeled input/output pairs
- Fits a straight line through training data
- Uses fitted line to predict future outcomes

## ğŸ” Visual Example

<img src="images/lin_reg_ex.png" alt="linear regression example" width="500"/>

## ğŸ’¡ Applications
- Housing price prediction
- Sales forecasting
- Consumer behavior analysis

## ğŸ“˜ Implementation Steps
### Single Variable Linear Regression:

#### 1. [Model Representation](single_variate/1_model_representation.ipynb)
#### Representing ğ‘“:

f<sub>w,b</sub>(x) = wx + b
- Determines output `Å·`
- `w` - Slope of line
- `b` - y-intercept

#### 2. [Cost Function](single_variate/2_cost_function.ipynb)
- <b>Squared Error Cost function</b>: Measures accuracy of predictions
```math
J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} (Å·â½â±â¾- yâ½â±â¾)Â²
```
  - `m` = number of training examples
  - `Å·â½â±â¾` = predicted value
  - `yâ½â±â¾` = actual value
- Goal: Minimize J(w,b)

#### 3. [Gradient Decent](single_variate/3_gradient_decent.ipynb)
- Have some cost function <b>J(w,b)</b>, want <b>$\min\limits_{w, b}$ J(w,b)</b>
- Takes down path of steepest decent to local minima

<br>

<b>Formula</b>:

tmp_w = w - $\alpha\;\frac{d}{dw}J(w,b)$ <br>
tmp_b = b - $\alpha\;\frac{d}{db}J(w,b)$ <br>
w = tmp_w <br>
b = tmp_b

<br>

#### Derivative of cost function with respect to w and to b: <br> <br>
 ### $\frac{d}{dw}J(w,b)$ = $\frac{1}{m}\sum\limits_{i=1}^{m}(\mathcal{f}_{w,b}(x^{(i)}-y^{(i)})x^{(i)})$ <br> <br>
 ### $\frac{d}{db}J(w,b)$ = $\frac{1}{m}\sum\limits_{i=1}^{m}(\mathcal{f}_{w,b}(x^{(i)}-y^{(i)}))$ <br> <br> 
- `Î±` = learning rate
  - if too small - greadient descent will be very slow
  - if too large - can overshoot the minimum and fail to converge (diverge)

`Must simultaneously update w and b `

<br>

### Multiple Linear Regression:
<img src="images/multi_lin_reg_ex.png" alt="mulitple 
linear regression example" width="500"/>
- $x_j$ = $j^{th}$ feature
- n = number of features
- $\vec{x}^{(i)}$ = features of $i^{th}$ training example
- $x_{j}^{(i)}$ = value of feature j in $i^{th}$ training example

#### 1. [Model Representation](multiple_linear_regression/multiple_lin_reg.ipynb)
### $\mathcal{f}_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 ... w_jx_j + b = \vec{w} \cdot \vec{x} + b$


Parameters:
- $\vec{w}$ = $[w_1\ w_2\ w_3\ ...w_j]$
- b

In numpy:
```python
f = np.dot(w,x) + b
```

#### 2. [Cost Function](multiple_linear_regression/multiple_lin_reg.ipynb)
### $J(\vec{w},b) = \frac{1}{2m} \sum\limits_{i=1}^{m} (\mathcal{f}_{\vec{w},b}(\vec{x}â½â±â¾- yâ½â±â¾)Â²$

#### 3. [Gradient Decent](multiple_linear_regression/multiple_lin_reg.ipynb)
### $w_1 = w_1 - \alpha\frac{1}{m} \sum\limits_{i=1}^{m} (\mathcal{f}_{\vec{w},b}(\vec{x}â½â±â¾- yâ½â±â¾)x_1^{(i)}$
. . .
### $w_n = w_n - \alpha\frac{1}{m} \sum\limits_{i=1}^{m} (\mathcal{f}_{\vec{w},b}(\vec{x}â½â±â¾- yâ½â±â¾)x_m^{(i)}$

### $b = b - \alpha\frac{1}{m} \sum\limits_{i=1}^{m} (\mathcal{f}_{\vec{w},b}(\vec{x}â½â±â¾- yâ½â±â¾)$

`w = np.array([0.5, 1.3, ... 3.4])` <br>
`d = np.array([0.3, 0.2, ... 0.4])` - derivatives <br>
compute $w_j = w_j - 0.1d_j$ for j=1....n

```python
# Vectorized in numpy
w = w - 0.1 * d
```
<br>

** `NORMAL EQUATION` **
- Alternative to gradient decent
- Only for linear regression
- Some libraries may use on backend
- Slow > 10k features


<br>

## [Feature Scaling](feature_scaling/README.md)

<img src="images/feature_scaling_ex.png" alt="mulitple 
linear regression example" width="500"/>

- If feature differ greatly (i.e. Feature 1 in 1s, Feature 2 in 1000s)
  - Plot axis will be skewed
  - Cause contour plot of cost function to be narrow
  - Slows gradient decent
- Scale features to be similiar values
  - Creates more circular contour plot
  - More direct path for gradient decent


<br>

## [Checking gradient decent for convergence]()
<img src="images/gd_convergence.png" alt="mulitple 
linear regression example" width="500"/>

#### If working properly:
- Gradient decent should decrease after each iteration

#### If J increases:
- Alpha chosen poorly
- Bug in code

#### # Iterations needed to converge varies

### Checking Convergence:
- Create learning curve graph and view
- Automatic convergence test: <br>
$ let \ \epsilon = 10^{-3} \ or \ 0.001 $ <br>
If $J(\vec{w},b)$ decreases by $\leq \epsilon$ in 1 iteration declare convergence

<br>

---

_Learning the relationships between variables to make predictions_